{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 1. SETUP AND AUTHENTICATION FOR GOOGLE COLAB ---\n",
    "# ==============================================================================\n",
    "\n",
    "# Install necessary libraries and download ExifTool\n",
    "!pip install pyexiftool webdavclient3 tqdm pandas rawpy opencv-python-headless numpy google-api-python-client google-auth-httplib2 google-auth-oauthlib > /dev/null\n",
    "!wget -q -O Image-ExifTool-13.34.tar.gz https://sourceforge.net/projects/exiftool/files/Image-ExifTool-13.34.tar.gz/download\n",
    "!tar -xzf Image-ExifTool-13.34.tar.gz\n",
    "!mv -f Image-ExifTool-13.34/* .\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import rawpy\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import exiftool\n",
    "import subprocess\n",
    "from io import BytesIO\n",
    "from datetime import datetime, timezone\n",
    "from webdav3 import client as wc\n",
    "\n",
    "# --- Third-party libraries ---\n",
    "from google.colab import auth\n",
    "from google.auth import default\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload, MediaIoBaseUpload\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Colab Specific Setup ---\n",
    "auth.authenticate_user()\n",
    "from google.colab import userdata\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "# --- Google Drive Settings ---\n",
    "SOURCE_ROOT_FOLDER_ID = userdata.get(\"SOURCE_ROOT_FOLDER_ID\")\n",
    "LOG_FOLDER_ID = userdata.get(\"LOG_FOLDER_ID\")\n",
    "\n",
    "# --- Nextcloud Settings (Destination) ---\n",
    "NEXTCLOUD_HOSTNAME = userdata.get(\"NEXTCLOUD_HOSTNAME\")\n",
    "NEXTCLOUD_USERNAME = userdata.get(\"NEXTCLOUD_USERNAME\")\n",
    "NEXTCLOUD_PASSWORD = userdata.get(\"NEXTCLOUD_PASSWORD\")\n",
    "NEXTCLOUD_ROOT_PATH = userdata.get(\"NEXTCLOUD_ROOT_PATH\")\n",
    "FAILED_API_FOLDER_NAME = \"FAILED\"\n",
    "\n",
    "# --- TBE API Settings ---\n",
    "TBE_API_URL = userdata.get(\"TBE_API_URL\")\n",
    "TBE_API_KEY = userdata.get(\"TBE_API_KEY\")\n",
    "\n",
    "# --- State Management ---\n",
    "MASTER_LOG_CSV = \"master_log.csv\"\n",
    "MAX_RUNTIME_SECONDS = 11 * 3600 + 45 * 60\n",
    "\n",
    "\n",
    "MONITORING_KEYWORDS = [\n",
    "    \"tikus\",\n",
    "    \"tanaman\",\n",
    "    \"ngengat\",\n",
    "    \"ulat\",\n",
    "    \"penggerek\"\n",
    "]\n",
    "MIN_JPEG_SIZE_MB = 1.0\n",
    "MAX_JPEG_SIZE_MB = 5.0\n",
    "\n",
    "# --- Required Metadata Keys ---\n",
    "REQUIRED_METADATA_KEYS = [\n",
    "    \"EXIF:GPSLatitude\",\n",
    "    \"EXIF:GPSLatitudeRef\",\n",
    "    \"EXIF:GPSLongitude\",\n",
    "    \"EXIF:GPSLongitudeRef\",\n",
    "    \"EXIF:GPSAltitude\",\n",
    "    \"XMP:AbsoluteAltitude\",\n",
    "    \"XMP:RelativeAltitude\",\n",
    "    \"XMP:GimbalYawDegree\",\n",
    "    \"XMP:GimbalPitchDegree\",\n",
    "    \"XMP:GimbalRollDegree\"\n",
    "]\n",
    "\n",
    "# --- Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. HELPER FUNCTIONS (Fungsi Bantuan) ---\n",
    "# ==============================================================================\n",
    "\n",
    "def extract_all_metadata_with_exiftool(dng_byte_stream, temp_dng_path=\"temp.dng\"):\n",
    "    lat, lon, all_metadata = None, None, {}\n",
    "    try:\n",
    "        with open(temp_dng_path, \"wb\") as f:\n",
    "            f.write(dng_byte_stream.getvalue())\n",
    "\n",
    "        with exiftool.ExifToolHelper(executable=\"./exiftool\") as et:\n",
    "            all_metadata = et.get_metadata(temp_dng_path)[0]\n",
    "\n",
    "        if \"EXIF:GPSLatitude\" in all_metadata and \"EXIF:GPSLongitude\" in all_metadata:\n",
    "            lat = all_metadata[\"EXIF:GPSLatitude\"]\n",
    "            lon = all_metadata[\"EXIF:GPSLongitude\"]\n",
    "            if 'S' in all_metadata.get('EXIF:GPSLatitudeRef', 'N'): lat = -lat\n",
    "            if 'W' in all_metadata.get('EXIF:GPSLongitudeRef', 'E'): lon = -lon\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting metadata with ExifTool: {e}\")\n",
    "    finally:\n",
    "        if os.path.exists(temp_dng_path):\n",
    "            os.remove(temp_dng_path)\n",
    "    return lat, lon, all_metadata\n",
    "\n",
    "def get_nextcloud_client():\n",
    "    options = {'webdav_hostname': NEXTCLOUD_HOSTNAME, 'webdav_login': NEXTCLOUD_USERNAME, 'webdav_password': NEXTCLOUD_PASSWORD}\n",
    "    try:\n",
    "        client = wc.Client(options); client.verify = True; return client\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize Nextcloud client: {e}\"); return None\n",
    "\n",
    "def ensure_nextcloud_folder(client, remote_path):\n",
    "    try:\n",
    "        if not client.check(remote_path): client.mkdir(remote_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to ensure Nextcloud folder '{remote_path}': {e}\"); return False\n",
    "\n",
    "def upload_to_nextcloud(jpeg_bytes, remote_path):\n",
    "    try:\n",
    "        jpeg_bytes.seek(0)\n",
    "        full_url = f\"{NEXTCLOUD_HOSTNAME}/{remote_path.lstrip('/')}\"\n",
    "        response = requests.put(full_url, data=jpeg_bytes, auth=(NEXTCLOUD_USERNAME, NEXTCLOUD_PASSWORD), headers={'Content-Type': 'image/jpeg'})\n",
    "        response.raise_for_status()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to upload to Nextcloud path '{remote_path}': {e}\"); return False\n",
    "\n",
    "def find_log_file_on_drive(service, folder_id, filename):\n",
    "    query = f\"'{folder_id}' in parents and name = '{filename}' and trashed = false\"\n",
    "    try:\n",
    "        response = service.files().list(q=query, spaces='drive', fields='files(id)').execute()\n",
    "        return response.get('files', [{}])[0].get('id')\n",
    "    except (HttpError, IndexError) as e:\n",
    "        logging.warning(f\"Could not find log file {filename} in Drive: {e}\"); return None\n",
    "\n",
    "def download_log_from_drive(service, file_id, local_path):\n",
    "    if not file_id: return False\n",
    "    try:\n",
    "        request = service.files().get_media(fileId=file_id)\n",
    "        fh = BytesIO()\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while not done: status, done = downloader.next_chunk()\n",
    "        with open(local_path, 'wb') as f: f.write(fh.getvalue())\n",
    "        logging.info(f\"Successfully downloaded log file from Drive to '{local_path}'.\")\n",
    "        return True\n",
    "    except HttpError as error:\n",
    "        logging.error(f\"Could not download log file: {error}\"); return False\n",
    "\n",
    "def update_log_on_drive(service, file_id, folder_id, local_path):\n",
    "    media = MediaFileUpload(local_path, mimetype='text/csv', resumable=True)\n",
    "    try:\n",
    "        if file_id:\n",
    "            service.files().update(fileId=file_id, media_body=media).execute()\n",
    "        else:\n",
    "            file_metadata = {'name': os.path.basename(local_path), 'parents': [folder_id]}\n",
    "            file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "            return file.get('id')\n",
    "    except HttpError as error:\n",
    "        logging.error(f\"Could not upload log file to Drive: {error}\")\n",
    "    return file_id\n",
    "\n",
    "def list_files_with_retry(service, query, page_token):\n",
    "    max_retries = 5; delay = 1\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return service.files().list(q=query, pageSize=1000, fields=\"nextPageToken, files(id, name, mimeType)\", pageToken=page_token).execute()\n",
    "        except HttpError as error:\n",
    "            if error.resp.status in [500, 502, 503, 504]:\n",
    "                logging.warning(f\"API call failed with transient error {error.resp.status}. Retrying in {delay}s...\")\n",
    "                time.sleep(delay); delay *= 2\n",
    "            else: raise\n",
    "    logging.error(f\"API call failed after {max_retries} retries for query: {query}\"); return None\n",
    "\n",
    "def traverse_drive(service, folder_id):\n",
    "    # Use actual source folder name instead of 'ROOT'\n",
    "    service2 = build('drive', 'v3', credentials=service._http.credentials)\n",
    "    folder_info = service2.files().get(fileId=folder_id, fields='name').execute()\n",
    "    source_folder_name = folder_info.get('name', 'ROOT')\n",
    "    folders_to_scan = [(folder_id, source_folder_name)]\n",
    "    while folders_to_scan:\n",
    "        current_id, current_path = folders_to_scan.pop(0)\n",
    "        query = f\"'{current_id}' in parents and trashed = false\"\n",
    "        page_token = None\n",
    "        while True:\n",
    "            results = list_files_with_retry(service, query, page_token)\n",
    "            if not results:\n",
    "                logging.error(f\"Could not list files in folder ID {current_id} after retries. Skipping folder.\"); break\n",
    "            for item in results.get('files', []):\n",
    "                item_path = os.path.join(current_path, item['name']).replace(\"\\\\\", \"/\")\n",
    "                if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
    "                    folders_to_scan.append((item['id'], item_path))\n",
    "                elif item['name'].lower().endswith('.dng'):\n",
    "                    yield item, os.path.splitext(item_path)[0]\n",
    "            page_token = results.get('nextPageToken')\n",
    "            if not page_token: break\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 4. CORE PROCESSING LOGIC (Logika Inti) ---\n",
    "# ==============================================================================\n",
    "\n",
    "def compress_dng_to_jpeg_bytes(dng_bytes):\n",
    "    \"\"\"Hanya melakukan kompresi, tanpa menangani metadata EXIF.\"\"\"\n",
    "    try:\n",
    "        dng_bytes.seek(0)\n",
    "        with rawpy.imread(dng_bytes) as raw:\n",
    "            rgb_image = raw.postprocess(use_camera_wb=True, output_bps=8)\n",
    "\n",
    "        bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
    "        high_quality = -1\n",
    "\n",
    "        for quality in range(95, 10, -5):\n",
    "            encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n",
    "            result, encoded_image_data = cv2.imencode('.jpeg', bgr_image, encode_param)\n",
    "            if not result: continue\n",
    "\n",
    "            file_size_mb = len(encoded_image_data) / (1024 * 1024)\n",
    "            if MIN_JPEG_SIZE_MB <= file_size_mb <= MAX_JPEG_SIZE_MB:\n",
    "                logging.info(f\"  > Compression success (coarse): Quality {quality} -> {file_size_mb:.2f} MB.\"); return BytesIO(encoded_image_data)\n",
    "\n",
    "            if file_size_mb > MAX_JPEG_SIZE_MB:\n",
    "                high_quality = quality\n",
    "            elif file_size_mb < MIN_JPEG_SIZE_MB and high_quality != -1:\n",
    "                low_quality = quality\n",
    "                logging.info(f\"  > Coarse search overshot. Starting fine search between {low_quality}-{high_quality}...\")\n",
    "                for fine_quality in range(high_quality - 1, low_quality, -1):\n",
    "                    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), fine_quality]\n",
    "                    result, encoded_image_data = cv2.imencode('.jpeg', bgr_image, encode_param)\n",
    "                    if not result: continue\n",
    "                    file_size_mb = len(encoded_image_data) / (1024 * 1024)\n",
    "                    if MIN_JPEG_SIZE_MB <= file_size_mb <= MAX_JPEG_SIZE_MB:\n",
    "                        logging.info(f\"  > Compression success (fine): Quality {fine_quality} -> {file_size_mb:.2f} MB.\"); return BytesIO(encoded_image_data)\n",
    "                logging.warning(f\"  > Fine search failed.\"); return None\n",
    "        logging.warning(\"  > Could not achieve target size.\"); return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"  > Failed during DNG compression: {e}\", exc_info=True); return None\n",
    "\n",
    "def inject_metadata_with_exiftool(dng_bytes, jpeg_bytes, temp_dng=\"temp.dng\", temp_jpeg=\"temp.jpeg\"):\n",
    "    \"\"\"Menyuntikkan semua metadata dari DNG ke JPEG menggunakan ExifTool.\"\"\"\n",
    "    try:\n",
    "        with open(temp_dng, \"wb\") as f:\n",
    "            f.write(dng_bytes.getvalue())\n",
    "        with open(temp_jpeg, \"wb\") as f:\n",
    "            f.write(jpeg_bytes.getvalue())\n",
    "\n",
    "        cmd = [\"./exiftool\", \"-tagsFromFile\", temp_dng, \"-all:all\", \"-overwrite_original\", temp_jpeg]\n",
    "        subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "\n",
    "        with open(temp_jpeg, \"rb\") as f:\n",
    "            final_jpeg_bytes = BytesIO(f.read())\n",
    "\n",
    "        return final_jpeg_bytes\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(f\"ExifTool failed to inject metadata: {e.stderr}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during metadata injection: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if os.path.exists(temp_dng): os.remove(temp_dng)\n",
    "        if os.path.exists(temp_jpeg): os.remove(temp_jpeg)\n",
    "\n",
    "def process_and_upload_file(gdrive_service, nextcloud_client, file_info, original_gdrive_path):\n",
    "    dng_id, original_filename = file_info['id'], file_info['name']\n",
    "    log_data = {\n",
    "        \"dng_id\": dng_id,\n",
    "        \"original_gdrive_path\": original_gdrive_path,\n",
    "        \"original_filename\": original_filename,\n",
    "        \"latitude\": None,\n",
    "        \"longitude\": None,\n",
    "        \"api_land_name\": None,\n",
    "        \"api_land_id\": None,\n",
    "        \"api_hst\": None,\n",
    "        \"api_hss\": None,\n",
    "        \"adjusted_hst\": None,\n",
    "        \"adjusted_hss\": None,\n",
    "        \"gdrive_uploaded_date\": None,\n",
    "        \"hst_hss_negative\": False,\n",
    "        \"api_response_json\": None,\n",
    "        \"final_nextcloud_path\": None,\n",
    "        \"status\": \"UNKNOWN_ERROR\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"exif_data_json\": None\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Get Google Drive uploaded date\n",
    "        file_metadata = gdrive_service.files().get(fileId=dng_id, fields='createdTime').execute()\n",
    "        gdrive_uploaded_date = file_metadata.get('createdTime', None)\n",
    "        log_data[\"gdrive_uploaded_date\"] = gdrive_uploaded_date\n",
    "        request = gdrive_service.files().get_media(fileId=dng_id)\n",
    "        dng_bytes = BytesIO(request.execute())\n",
    "    except HttpError as e:\n",
    "        logging.error(f\"Failed to download {original_filename}: {e}\"); log_data[\"status\"] = \"DOWNLOAD_FAIL\"; return log_data\n",
    "\n",
    "    lat, lon, all_exif_data = extract_all_metadata_with_exiftool(dng_bytes)\n",
    "    missing_keys = [key for key in REQUIRED_METADATA_KEYS if key not in all_exif_data]\n",
    "    if missing_keys:\n",
    "        logging.warning(f\"Skipping {original_filename} due to missing required metadata: {missing_keys}\")\n",
    "        log_data[\"status\"] = \"MISSING_METADATA_FAIL\"\n",
    "        log_data[\"exif_data_json\"] = json.dumps(all_exif_data)\n",
    "        return log_data\n",
    "    if not all([lat, lon]):\n",
    "        logging.warning(f\"Skipping {original_filename} due to missing GPS metadata.\"); log_data[\"status\"] = \"NO_METADATA\"; return log_data\n",
    "    log_data.update({\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"exif_data_json\": json.dumps(all_exif_data)\n",
    "    })\n",
    "\n",
    "    compressed_jpeg_bytes = compress_dng_to_jpeg_bytes(dng_bytes)\n",
    "    if not compressed_jpeg_bytes:\n",
    "        logging.error(f\"Failed to compress {original_filename}. Skipping.\"); log_data[\"status\"] = \"COMPRESS_FAIL\"; return log_data\n",
    "\n",
    "    final_jpeg_bytes = inject_metadata_with_exiftool(dng_bytes, compressed_jpeg_bytes)\n",
    "    if not final_jpeg_bytes:\n",
    "        logging.error(f\"Failed to inject metadata into {original_filename}. Skipping.\"); log_data[\"status\"] = \"METADATA_INJECT_FAIL\"; return log_data\n",
    "\n",
    "    new_filename_part = original_gdrive_path.replace('/', '_')\n",
    "    try:\n",
    "        params, headers = {'lat': lat, 'lng': lon}, {'Key': TBE_API_KEY}\n",
    "        response = requests.get(TBE_API_URL, params=params, headers=headers, timeout=20)\n",
    "        log_data[\"api_response_json\"] = json.dumps(response.json())\n",
    "        response.raise_for_status()\n",
    "\n",
    "        if response.json().get('status') is True and response.json().get('data', {}).get('land_id'):\n",
    "            api_data = response.json()['data']\n",
    "            land_name = api_data.get('land_name', None)\n",
    "            land_id = api_data['land_id']\n",
    "            hst = int(api_data.get('land_hst', '0'))\n",
    "            hss = int(api_data.get('land_hss', '0'))\n",
    "            log_data[\"api_land_name\"] = land_name\n",
    "            log_data[\"api_land_id\"] = land_id\n",
    "            log_data[\"api_hst\"] = hst\n",
    "            log_data[\"api_hss\"] = hss\n",
    "            # Calculate days since upload\n",
    "            days_since_upload = 0\n",
    "            if gdrive_uploaded_date:\n",
    "                try:\n",
    "                    uploaded_dt = datetime.strptime(gdrive_uploaded_date, \"%Y-%m-%dT%H:%M:%S.%fZ\").replace(tzinfo=timezone.utc)\n",
    "                    now_dt = datetime.now(timezone.utc)\n",
    "                    days_since_upload = (now_dt - uploaded_dt).days\n",
    "                except Exception as ex:\n",
    "                    logging.warning(f\"Could not parse gdrive_uploaded_date: {gdrive_uploaded_date}, error: {ex}\")\n",
    "            # Adjust hst/hss only if not zero\n",
    "            adjusted_hst = hst - days_since_upload if hst != 0 else 0\n",
    "            adjusted_hss = hss - days_since_upload if hss != 0 else 0\n",
    "            log_data[\"adjusted_hst\"] = adjusted_hst\n",
    "            log_data[\"adjusted_hss\"] = adjusted_hss\n",
    "            if adjusted_hst < 0 or adjusted_hss < 0:\n",
    "                log_data[\"hst_hss_negative\"] = True\n",
    "                logging.warning(f\"Adjusted hst/hss is negative for file {original_filename}: adjusted_hst={adjusted_hst}, adjusted_hss={adjusted_hss}\")\n",
    "                log_data[\"status\"] = \"NEGATIVE_HST_HSS_SKIP\"\n",
    "                return log_data\n",
    "            # Use adjusted value for foldering\n",
    "            day_val = str(adjusted_hst) if adjusted_hst != 0 else str(adjusted_hss)\n",
    "            main_folder_path = f\"{NEXTCLOUD_ROOT_PATH}/{land_id}-{day_val}\"\n",
    "            if ensure_nextcloud_folder(nextcloud_client, main_folder_path):\n",
    "                for keyword in MONITORING_KEYWORDS: ensure_nextcloud_folder(nextcloud_client, f\"{main_folder_path}/monitoring {keyword}\")\n",
    "                target_subfolder = next((f\"monitoring {kw}\" for kw in MONITORING_KEYWORDS if kw in original_gdrive_path.lower()), \"monitoring tanaman\")\n",
    "                final_path = f\"{main_folder_path}/{target_subfolder}/{new_filename_part}.jpeg\"\n",
    "                log_data[\"final_nextcloud_path\"] = final_path\n",
    "                log_data[\"status\"] = \"SUCCESS\" if upload_to_nextcloud(final_jpeg_bytes, final_path) else \"UPLOAD_FAIL\"\n",
    "            else:\n",
    "                log_data[\"status\"] = \"NEXTCLOUD_FOLDER_FAIL\"\n",
    "            return log_data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"API request failed for {original_filename}: {e}\"); log_data[\"api_response_json\"] = json.dumps({\"error\": str(e)})\n",
    "\n",
    "    # Fallback untuk kegagalan API\n",
    "    logging.warning(f\"API call failed for {original_filename}. Uploading to FAILED folder.\")\n",
    "    log_data[\"api_land_id\"] = \"API_FAIL\"\n",
    "    failure_path = f\"{NEXTCLOUD_ROOT_PATH}/{FAILED_API_FOLDER_NAME}\"\n",
    "    ensure_nextcloud_folder(nextcloud_client, failure_path)\n",
    "    final_path = f\"{failure_path}/{new_filename_part}.jpeg\"\n",
    "    log_data[\"final_nextcloud_path\"] = final_path\n",
    "    log_data[\"status\"] = \"API_FAIL_UPLOADED\" if upload_to_nextcloud(final_jpeg_bytes, final_path) else \"API_FAIL_UPLOAD_FAIL\"\n",
    "    return log_data\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 5. MAIN EXECUTION (Eksekusi Utama) ---\n",
    "# ==============================================================================\n",
    "def main():\n",
    "    start_time = time.monotonic()\n",
    "    creds, _ = default()\n",
    "    gdrive_service = build('drive', 'v3', credentials=creds)\n",
    "    nextcloud_client = get_nextcloud_client()\n",
    "    if not nextcloud_client:\n",
    "        logging.critical(\"Could not connect to Nextcloud. Aborting.\"); return\n",
    "\n",
    "    log_file_id = find_log_file_on_drive(gdrive_service, LOG_FOLDER_ID, MASTER_LOG_CSV)\n",
    "    download_log_from_drive(gdrive_service, log_file_id, MASTER_LOG_CSV)\n",
    "\n",
    "\n",
    "    # Load master log and show status counts\n",
    "    if os.path.exists(MASTER_LOG_CSV) and os.path.getsize(MASTER_LOG_CSV) > 0:\n",
    "        master_log_df = pd.read_csv(MASTER_LOG_CSV)\n",
    "    else:\n",
    "        master_log_df = pd.DataFrame(columns=[\n",
    "            \"dng_id\", \"original_gdrive_path\", \"original_filename\", \"latitude\", \"longitude\",\n",
    "            \"api_land_name\", \"api_land_id\", \"api_hst\", \"api_hss\", \"adjusted_hst\", \"adjusted_hss\",\n",
    "            \"gdrive_uploaded_date\", \"hst_hss_negative\", \"api_response_json\", \"final_nextcloud_path\",\n",
    "            \"status\", \"timestamp\", \"exif_data_json\"\n",
    "        ])\n",
    "\n",
    "    # Show status counts before processing\n",
    "    status_counts = master_log_df['status'].value_counts()\n",
    "    logging.info(\"Status counts in master log before processing:\")\n",
    "    for status, count in status_counts.items():\n",
    "        logging.info(f\"  {status}: {count}\")\n",
    "\n",
    "    # Define statuses to skip reprocessing\n",
    "    skip_statuses = [\"SUCCESS\", \"API_FAIL_UPLOADED\", \"NEGATIVE_HST_HSS_SKIP\"]\n",
    "\n",
    "    # Get all DNG files from Drive\n",
    "    all_tasks = list(traverse_drive(gdrive_service, SOURCE_ROOT_FOLDER_ID))\n",
    "    # Build a dict for quick lookup by dng_id\n",
    "    all_tasks_dict = {task[0]['id']: task for task in all_tasks}\n",
    "\n",
    "    # Find DNGs to reprocess (status not in skip_statuses)\n",
    "    to_reprocess_ids = set(master_log_df[~master_log_df['status'].isin(skip_statuses)]['dng_id'].astype(str))\n",
    "    reprocess_tasks = [all_tasks_dict[dng_id] for dng_id in to_reprocess_ids if dng_id in all_tasks_dict]\n",
    "\n",
    "    # Find DNGs never processed (not in master log)\n",
    "    processed_dng_ids = set(master_log_df['dng_id'].astype(str))\n",
    "    new_tasks = [task for task in all_tasks if task[0]['id'] not in processed_dng_ids]\n",
    "\n",
    "    # Combine: reprocess first, then new\n",
    "    tasks_to_run = reprocess_tasks + new_tasks\n",
    "    logging.info(f\"Found {len(all_tasks)} total DNG files. {len(tasks_to_run)} files to process (including reprocess).\")\n",
    "\n",
    "    for file_info, original_gdrive_path in tqdm(tasks_to_run, desc=\"Processing DNG Files\"):\n",
    "        if time.monotonic() - start_time > MAX_RUNTIME_SECONDS:\n",
    "            logging.warning(\"Runtime limit reached. Stopping script.\"); break\n",
    "\n",
    "        result_log = process_and_upload_file(gdrive_service, nextcloud_client, file_info, original_gdrive_path)\n",
    "\n",
    "        dng_id_to_update = result_log['dng_id']\n",
    "        master_log_df = master_log_df[master_log_df['dng_id'] != dng_id_to_update]\n",
    "\n",
    "        new_row_df = pd.DataFrame([result_log])\n",
    "        master_log_df = pd.concat([master_log_df, new_row_df], ignore_index=True)\n",
    "        master_log_df.to_csv(MASTER_LOG_CSV, index=False)\n",
    "\n",
    "        log_file_id = update_log_on_drive(gdrive_service, log_file_id, LOG_FOLDER_ID, MASTER_LOG_CSV)\n",
    "\n",
    "    logging.info(\"âœ… Processing complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

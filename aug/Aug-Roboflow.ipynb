{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eec6698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d1f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa919b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"ultralytics<=8.3.40\" supervision roboflow\n",
    "# prevent ultralytics from tracking your activity\n",
    "!yolo settings sync=False\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model=yolo11m-seg.pt conf=0.25 source='https://media.roboflow.com/notebooks/examples/dog.jpeg' save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "IPyImage(filename=f'{HOME}/runs/segment/predict/dog.jpg', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86266d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "model = YOLO('yolo11m-seg.pt')\n",
    "image = Image.open(requests.get('https://media.roboflow.com/notebooks/examples/dog.jpeg', stream=True).raw)\n",
    "result = model.predict(image, conf=0.25)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583ba00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "\n",
    "!roboflow workspace list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {HOME}/datasets\n",
    "%cd {HOME}/datasets\n",
    "\n",
    "from google.colab import userdata\n",
    "from roboflow import Roboflow\n",
    "\n",
    "ROBOFLOW_API_KEY = userdata.get('ROBOFLOW_API_KEY')\n",
    "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
    "\n",
    "workspace = rf.workspace(\"tbe\") # Your workspace ID\n",
    "project = workspace.project(\"rice-grain-svjri\") # Your project ID\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"yolov11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b4612",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {HOME}\n",
    "\n",
    "!yolo task=segment mode=train model=yolo11m-seg.pt data={dataset.location}/data.yaml epochs=1000 imgsz=640 plots=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {HOME}/runs/segment/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9605e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image as IPyImage\n",
    "\n",
    "# IPyImage(filename=f'{HOME}/runs/segment/train/confusion_matrix.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=segment mode=val model={HOME}/runs/segment/train/weights/best.pt data={dataset.location}/data.yaml imgsz=640 plots=True max_det=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d77c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "# Melakukan augmentasi pada dataset untuk meningkatkan variasi data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1384ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install library untuk augmentasi\n",
    "%pip install albumentations opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81798dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Fungsi untuk membaca annotation YOLO format\n",
    "def read_yolo_annotation(annotation_path):\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        annotations = []\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) > 0:\n",
    "                class_id = int(parts[0])\n",
    "                coords = [float(x) for x in parts[1:]]\n",
    "                annotations.append([class_id] + coords)\n",
    "    return annotations\n",
    "\n",
    "# Fungsi untuk menulis annotation YOLO format\n",
    "def write_yolo_annotation(annotation_path, annotations):\n",
    "    with open(annotation_path, 'w') as f:\n",
    "        for ann in annotations:\n",
    "            class_id = int(ann[0])\n",
    "            coords = ' '.join([f'{x:.6f}' for x in ann[1:]])\n",
    "            f.write(f'{class_id} {coords}\\n')\n",
    "\n",
    "# Fungsi untuk membaca konfigurasi dataset dari YAML\n",
    "def load_data_config(data_yaml_path, dataset_path_hint=None):\n",
    "    data_yaml_path = Path(data_yaml_path)\n",
    "    with open(data_yaml_path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "\n",
    "    names = data.get('names', [])\n",
    "    if isinstance(names, dict):\n",
    "        names = [names[str(i)] for i in range(len(names))]\n",
    "\n",
    "    # Gunakan dataset_path_hint sebagai base jika diberikan, jika tidak gunakan parent dari data.yaml\n",
    "    if dataset_path_hint:\n",
    "        base_dir = Path(dataset_path_hint).resolve()\n",
    "    else:\n",
    "        base_dir = data_yaml_path.parent.resolve()\n",
    "\n",
    "    def resolve_path(path_value: str | Path):\n",
    "        path_value = Path(path_value)\n",
    "        # Jika absolut, kembalikan langsung\n",
    "        if path_value.is_absolute():\n",
    "            return path_value.resolve()\n",
    "        # Jika relatif, gabungkan dengan base_dir dan resolve\n",
    "        # Normalisasi: hilangkan .. dan . dari path\n",
    "        parts = []\n",
    "        for part in path_value.parts:\n",
    "            if part == '..':\n",
    "                if parts:\n",
    "                    parts.pop()\n",
    "            elif part != '.':\n",
    "                parts.append(part)\n",
    "        normalized = Path(*parts) if parts else Path('.')\n",
    "        resolved = (base_dir / normalized).resolve()\n",
    "        return resolved\n",
    "\n",
    "    splits = {}\n",
    "    for split_key in ('train', 'val', 'test'):\n",
    "        split_path = data.get(split_key)\n",
    "        if not split_path:\n",
    "            continue\n",
    "        images_dir = resolve_path(split_path)\n",
    "        # Labels biasanya sejajar dengan images di parent/labels\n",
    "        labels_dir = (images_dir.parent / 'labels').resolve()\n",
    "        splits[split_key] = {\n",
    "            'images': images_dir,\n",
    "            'labels': labels_dir,\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'names': names,\n",
    "        'splits': splits,\n",
    "    }\n",
    "\n",
    "# Menghitung distribusi kelas dan statistik per gambar\n",
    "def collect_class_stats(labels_dir, num_classes):\n",
    "    labels_dir = Path(labels_dir)\n",
    "    counts = Counter({cls: 0 for cls in range(num_classes)})\n",
    "    image_stats = {}\n",
    "\n",
    "    for label_path in labels_dir.glob('*.txt'):\n",
    "        annotations = read_yolo_annotation(label_path)\n",
    "        per_image = Counter({cls: 0 for cls in range(num_classes)})\n",
    "        for ann in annotations:\n",
    "            class_id = int(ann[0])\n",
    "            if class_id >= num_classes:\n",
    "                continue\n",
    "            per_image[class_id] += 1\n",
    "            counts[class_id] += 1\n",
    "        image_stats[label_path.stem] = per_image\n",
    "\n",
    "    return counts, image_stats\n",
    "\n",
    "# Menampilkan distribusi kelas yang mudah dibaca\n",
    "def print_class_distribution(split_name, counts, class_names):\n",
    "    total = sum(counts.values())\n",
    "    print(f\"\\nDistribusi kelas untuk {split_name}:\")\n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        value = counts.get(idx, 0)\n",
    "        if total > 0:\n",
    "            pct = (value / total) * 100\n",
    "            print(f\"  - {class_name}: {value} ({pct:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"  - {class_name}: {value}\")\n",
    "\n",
    "# Menentukan rencana augmentasi agar kelas minoritas mendekati jumlah kelas mayoritas\n",
    "def build_balanced_augmentation_plan(image_stats, target_class_id, deficit, max_aug_per_image=5):\n",
    "    plan = {}\n",
    "    if deficit <= 0:\n",
    "        return plan\n",
    "\n",
    "    eligible = []\n",
    "    for stem, stats in image_stats.items():\n",
    "        instances = stats.get(target_class_id, 0)\n",
    "        if instances > 0:\n",
    "            eligible.append((stem, instances))\n",
    "\n",
    "    if not eligible:\n",
    "        return plan\n",
    "\n",
    "    eligible.sort(key=lambda item: item[1], reverse=True)\n",
    "    total_capacity = sum(instances * max_aug_per_image for _, instances in eligible)\n",
    "    if total_capacity < deficit:\n",
    "        print(\n",
    "            f\"Peringatan: kapasitas augmentasi maksimum ({total_capacity}) lebih kecil dari kebutuhan ({deficit}). \"\n",
    "            \"Dataset mungkin tetap tidak seimbang.\"\n",
    "        )\n",
    "\n",
    "    idx = 0\n",
    "    iterations = 0\n",
    "    max_iterations = len(eligible) * max_aug_per_image if eligible else 0\n",
    "\n",
    "    while deficit > 0 and idx < max_iterations and eligible:\n",
    "        stem, instances = eligible[idx % len(eligible)]\n",
    "        if plan.get(stem, 0) >= max_aug_per_image:\n",
    "            idx += 1\n",
    "            iterations += 1\n",
    "            continue\n",
    "\n",
    "        plan[stem] = plan.get(stem, 0) + 1\n",
    "        deficit -= instances\n",
    "        idx += 1\n",
    "        iterations += 1\n",
    "\n",
    "    if deficit > 0:\n",
    "        print(f\"Peringatan: Masih ada selisih {deficit} instance setelah perencanaan augmentasi.\")\n",
    "\n",
    "    return plan\n",
    "\n",
    "# Mengambil patch objek dari polygon YOLO (segmentation)\n",
    "def extract_object_patch(image, polygon_coords, padding=2):\n",
    "    height, width = image.shape[:2]\n",
    "    if len(polygon_coords) < 6:\n",
    "        return None\n",
    "\n",
    "    pts = np.array(polygon_coords, dtype=np.float32).reshape(-1, 2)\n",
    "    x_px = np.clip(np.round(pts[:, 0] * width), 0, width - 1)\n",
    "    y_px = np.clip(np.round(pts[:, 1] * height), 0, height - 1)\n",
    "    pts_px = np.stack([x_px, y_px], axis=1).astype(np.int32)\n",
    "\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, [pts_px], 1)\n",
    "\n",
    "    x_min = max(0, int(np.min(pts_px[:, 0])) - padding)\n",
    "    x_max = min(width, int(np.max(pts_px[:, 0])) + padding)\n",
    "    y_min = max(0, int(np.min(pts_px[:, 1])) - padding)\n",
    "    y_max = min(height, int(np.max(pts_px[:, 1])) + padding)\n",
    "\n",
    "    if x_max - x_min < 2 or y_max - y_min < 2:\n",
    "        return None\n",
    "\n",
    "    patch = image[y_min:y_max, x_min:x_max]\n",
    "    mask_patch = mask[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    if mask_patch.max() == 0:\n",
    "        return None\n",
    "\n",
    "    polygon_local = pts_px - np.array([x_min, y_min], dtype=np.int32)\n",
    "    return patch, mask_patch, polygon_local\n",
    "\n",
    "# Menempelkan patch pada gambar target dan mengembalikan polygon baru\n",
    "def paste_patch_on_base(base_image, patch, mask_patch, polygon_local, sigma=3):\n",
    "    base_height, base_width = base_image.shape[:2]\n",
    "    patch_height, patch_width = patch.shape[:2]\n",
    "\n",
    "    if patch_height == 0 or patch_width == 0:\n",
    "        return None\n",
    "\n",
    "    if patch_height > base_height or patch_width > base_width:\n",
    "        return None\n",
    "\n",
    "    max_x = base_width - patch_width\n",
    "    max_y = base_height - patch_height\n",
    "\n",
    "    if max_x < 0 or max_y < 0:\n",
    "        return None\n",
    "\n",
    "    if max_x == 0 and max_y == 0:\n",
    "        x_offset, y_offset = 0, 0\n",
    "    else:\n",
    "        x_offset = random.randint(0, max_x)\n",
    "        y_offset = random.randint(0, max_y)\n",
    "\n",
    "    mask_float = mask_patch.astype(np.float32)\n",
    "    if mask_float.max() == 0:\n",
    "        return None\n",
    "    mask_float /= mask_float.max()\n",
    "\n",
    "    if sigma and sigma > 0:\n",
    "        mask_float = cv2.GaussianBlur(mask_float, (0, 0), sigmaX=sigma, sigmaY=sigma)\n",
    "    mask_float = np.clip(mask_float, 0.0, 1.0)\n",
    "    mask_float = mask_float[..., None]\n",
    "\n",
    "    roi = base_image[y_offset:y_offset + patch_height, x_offset:x_offset + patch_width]\n",
    "    blended = (mask_float * patch.astype(np.float32) + (1.0 - mask_float) * roi.astype(np.float32)).astype(np.uint8)\n",
    "    base_image[y_offset:y_offset + patch_height, x_offset:x_offset + patch_width] = blended\n",
    "\n",
    "    polygon_shifted = polygon_local + np.array([x_offset, y_offset], dtype=np.int32)\n",
    "    polygon_norm = []\n",
    "    for x_px, y_px in polygon_shifted:\n",
    "        polygon_norm.append(float(np.clip(x_px / base_width, 0.0, 1.0)))\n",
    "        polygon_norm.append(float(np.clip(y_px / base_height, 0.0, 1.0)))\n",
    "\n",
    "    return polygon_norm\n",
    "\n",
    "print(\"Fungsi utilitas augmentasi copy-paste berhasil didefinisikan!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89a79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformasi warna/pixel opsional setelah copy-paste\n",
    "import albumentations as A\n",
    "\n",
    "transform_color = A.Compose([\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=0.2, \n",
    "        contrast_limit=0.2, \n",
    "        p=0.5\n",
    "    ),\n",
    "    A.HueSaturationValue(\n",
    "        hue_shift_limit=10,\n",
    "        sat_shift_limit=20,\n",
    "        val_shift_limit=10,\n",
    "        p=0.5\n",
    "    ),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "    A.CLAHE(clip_limit=2.0, p=0.3),\n",
    "], bbox_params=None)\n",
    "\n",
    "print(\"Pipeline augmentasi warna berhasil didefinisikan (opsional setelah copy-paste)!\")\n",
    "print(\"Transformasi warna/pixel yang digunakan:\")\n",
    "print(\"  1. RandomBrightnessContrast (50%)\")\n",
    "print(\"  2. HueSaturationValue (50%)\")\n",
    "print(\"  3. GaussNoise (30%)\")\n",
    "print(\"  4. GaussianBlur (30%)\")\n",
    "print(\"  5. CLAHE (30%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980fc7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisasi hasil augmentasi\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def visualize_augmentations(images_path, num_samples=4):\n",
    "    \"\"\"Visualisasi gambar original dan hasil augmentasinya.\"\"\"\n",
    "    images_dir = Path(images_path)\n",
    "    if not images_dir.exists():\n",
    "        print(f\"Direktori gambar tidak ditemukan: {images_dir}\")\n",
    "        print(\"Pastikan cell konfigurasi berjalan dan augmentasi menghasilkan gambar.\")\n",
    "        return\n",
    "\n",
    "    # Filter: gambar original adalah yang TIDAK mengandung '_cp_' atau '_aug' di filename\n",
    "    original_images = [\n",
    "        f.name\n",
    "        for f in images_dir.iterdir()\n",
    "        if f.is_file() \n",
    "        and '_cp_' not in f.stem \n",
    "        and '_aug' not in f.stem \n",
    "        and f.suffix.lower() in {'.jpg', '.jpeg', '.png'}\n",
    "    ]\n",
    "\n",
    "    if len(original_images) == 0:\n",
    "        print(\"Tidak ada gambar original ditemukan\")\n",
    "        return\n",
    "\n",
    "    sample_images = random.sample(original_images, min(num_samples, len(original_images)))\n",
    "\n",
    "    for orig_name in sample_images:\n",
    "        base_name = Path(orig_name).stem\n",
    "        # Cari gambar copy-paste yang berasal dari gambar original ini\n",
    "        # Format: {base_name}_cp_*\n",
    "        aug_images = [\n",
    "            f.name\n",
    "            for f in images_dir.iterdir()\n",
    "            if f.is_file() \n",
    "            and f.name.startswith(base_name + '_cp_') \n",
    "            and f.suffix.lower() in {'.jpg', '.jpeg', '.png'}\n",
    "        ]\n",
    "\n",
    "        if len(aug_images) == 0:\n",
    "            print(f\"Tidak ada hasil copy-paste untuk {orig_name}.\")\n",
    "            continue\n",
    "\n",
    "        num_cols = min(4, len(aug_images) + 1)\n",
    "        fig, axes = plt.subplots(1, num_cols, figsize=(20, 5))\n",
    "\n",
    "        if num_cols == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        orig_img = Image.open(images_dir / orig_name)\n",
    "        axes[0].imshow(orig_img)\n",
    "        axes[0].set_title('Original')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        for idx, aug_name in enumerate(aug_images[:num_cols-1]):\n",
    "            aug_img = Image.open(images_dir / aug_name)\n",
    "            axes[idx+1].imshow(aug_img)\n",
    "            axes[idx+1].set_title(f'Copy-Paste {idx+1}')\n",
    "            axes[idx+1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f\"Gambar: {orig_name}\")\n",
    "        print(f\"  - Jumlah augmentasi: {len(aug_images)}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# NOTE: pemanggilan visualize_augmentations dilakukan setelah proses augmentasi selesai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d9f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurasi path dan parameter copy-paste balancing\n",
    "from pathlib import Path\n",
    "\n",
    "# Gunakan path dataset dari hasil download Roboflow jika tersedia\n",
    "if 'dataset' in globals():\n",
    "    DATASET_PATH = Path(dataset.location).resolve()\n",
    "else:\n",
    "    DATASET_PATH = Path(\"/content/datasets/Rice-Grain-4\").resolve()\n",
    "\n",
    "DATA_CONFIG_PATH = (DATASET_PATH / \"data.yaml\").resolve()\n",
    "if not DATA_CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f\"data.yaml tidak ditemukan di {DATA_CONFIG_PATH}. Pastikan dataset sudah diunduh.\")\n",
    "\n",
    "data_config = load_data_config(DATA_CONFIG_PATH, dataset_path_hint=DATASET_PATH)\n",
    "CLASS_NAMES = data_config[\"names\"]\n",
    "SPLITS = data_config[\"splits\"]\n",
    "\n",
    "if \"train\" not in SPLITS:\n",
    "    raise ValueError(\"Path train tidak ditemukan pada data.yaml. Pastikan konfigurasi dataset benar.\")\n",
    "\n",
    "TRAIN_IMAGES_PATH = SPLITS[\"train\"][\"images\"]\n",
    "TRAIN_LABELS_PATH = SPLITS[\"train\"][\"labels\"]\n",
    "VAL_IMAGES_PATH = SPLITS.get(\"val\", {}).get(\"images\")\n",
    "VAL_LABELS_PATH = SPLITS.get(\"val\", {}).get(\"labels\")\n",
    "TEST_IMAGES_PATH = SPLITS.get(\"test\", {}).get(\"images\")\n",
    "TEST_LABELS_PATH = SPLITS.get(\"test\", {}).get(\"labels\")\n",
    "\n",
    "# Verifikasi path yang dihasilkan\n",
    "for name, path_value in [\n",
    "    (\"TRAIN_IMAGES_PATH\", TRAIN_IMAGES_PATH),\n",
    "    (\"TRAIN_LABELS_PATH\", TRAIN_LABELS_PATH),\n",
    "    (\"VAL_IMAGES_PATH\", VAL_IMAGES_PATH),\n",
    "    (\"VAL_LABELS_PATH\", VAL_LABELS_PATH),\n",
    "    (\"TEST_IMAGES_PATH\", TEST_IMAGES_PATH),\n",
    "    (\"TEST_LABELS_PATH\", TEST_LABELS_PATH),\n",
    "]:\n",
    "    if path_value is None:\n",
    "        continue\n",
    "    if not path_value.exists():\n",
    "        print(f\"Peringatan: {name} tidak ditemukan di {path_value}\")\n",
    "\n",
    "# Parameter balancing berbasis copy-paste\n",
    "TARGET_MINORITY_CLASS_NAME = \"brown_spot\"\n",
    "DEFAULT_BASE_AUGMENTATIONS = 0      # augmentasi dasar untuk semua gambar\n",
    "MAX_AUG_PER_IMAGE = 5               # batas augmentasi tambahan per gambar minoritas\n",
    "COPY_PASTE_MIN_OBJECTS = 1          # minimal objek minoritas yang ditempel per gambar baru\n",
    "COPY_PASTE_MAX_OBJECTS = 3          # maksimal objek minoritas yang ditempel per gambar baru\n",
    "COPY_PASTE_PADDING = 4              # padding di sekitar mask saat memotong objek\n",
    "MASK_BLUR_SIGMA = 3                 # smoothing tepi saat penempelan\n",
    "APPLY_COLOR_AUG = True              # gunakan transformasi warna setelah copy-paste\n",
    "\n",
    "if COPY_PASTE_MAX_OBJECTS < COPY_PASTE_MIN_OBJECTS:\n",
    "    raise ValueError(\"COPY_PASTE_MAX_OBJECTS harus >= COPY_PASTE_MIN_OBJECTS\")\n",
    "\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Train images path: {TRAIN_IMAGES_PATH}\")\n",
    "print(f\"Train labels path: {TRAIN_LABELS_PATH}\")\n",
    "if VAL_IMAGES_PATH:\n",
    "    print(f\"Validation images path: {VAL_IMAGES_PATH}\")\n",
    "if TEST_IMAGES_PATH:\n",
    "    print(f\"Test images path: {TEST_IMAGES_PATH}\")\n",
    "print(f\"Jumlah kelas: {len(CLASS_NAMES)} -> {CLASS_NAMES}\")\n",
    "print(\"Parameter copy-paste:\")\n",
    "print(f\"  - Target kelas minoritas : {TARGET_MINORITY_CLASS_NAME}\")\n",
    "print(f\"  - Min objek per augmentasi: {COPY_PASTE_MIN_OBJECTS}\")\n",
    "print(f\"  - Max objek per augmentasi: {COPY_PASTE_MAX_OBJECTS}\")\n",
    "print(f\"  - Padding objek          : {COPY_PASTE_PADDING}\")\n",
    "print(f\"  - Mask blur sigma        : {MASK_BLUR_SIGMA}\")\n",
    "print(f\"  - Color augment aktif    : {APPLY_COLOR_AUG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17400eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisis distribusi kelas & rencana balancing\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "train_counts, TRAIN_IMAGE_STATS = collect_class_stats(TRAIN_LABELS_PATH, NUM_CLASSES)\n",
    "val_counts = Counter({cls: 0 for cls in range(NUM_CLASSES)})\n",
    "test_counts = Counter({cls: 0 for cls in range(NUM_CLASSES)})\n",
    "\n",
    "if VAL_LABELS_PATH:\n",
    "    val_counts, _ = collect_class_stats(VAL_LABELS_PATH, NUM_CLASSES)\n",
    "if TEST_LABELS_PATH:\n",
    "    test_counts, _ = collect_class_stats(TEST_LABELS_PATH, NUM_CLASSES)\n",
    "\n",
    "print_class_distribution(\"Train (sebelum augmentasi)\", train_counts, CLASS_NAMES)\n",
    "if VAL_LABELS_PATH:\n",
    "    print_class_distribution(\"Validation\", val_counts, CLASS_NAMES)\n",
    "if TEST_LABELS_PATH:\n",
    "    print_class_distribution(\"Test\", test_counts, CLASS_NAMES)\n",
    "\n",
    "if TARGET_MINORITY_CLASS_NAME in CLASS_NAMES:\n",
    "    minority_class_id = CLASS_NAMES.index(TARGET_MINORITY_CLASS_NAME)\n",
    "else:\n",
    "    minority_class_id = min(train_counts, key=train_counts.get)\n",
    "majority_class_id = max(train_counts, key=train_counts.get)\n",
    "\n",
    "balance_deficit = train_counts[majority_class_id] - train_counts[minority_class_id]\n",
    "\n",
    "print(\"\\nRingkasan balancing:\")\n",
    "print(f\"Kelas mayoritas : {CLASS_NAMES[majority_class_id]} ({train_counts[majority_class_id]} instance)\")\n",
    "print(f\"Kelas minoritas : {CLASS_NAMES[minority_class_id]} ({train_counts[minority_class_id]} instance)\")\n",
    "print(f\"Selisih instance : {balance_deficit}\")\n",
    "\n",
    "AUGMENTATION_PLAN = build_balanced_augmentation_plan(\n",
    "    TRAIN_IMAGE_STATS,\n",
    "    target_class_id=minority_class_id,\n",
    "    deficit=balance_deficit,\n",
    "    max_aug_per_image=MAX_AUG_PER_IMAGE,\n",
    ")\n",
    "\n",
    "expected_new_minority = sum(\n",
    "    TRAIN_IMAGE_STATS[stem].get(minority_class_id, 0) * count\n",
    "    for stem, count in AUGMENTATION_PLAN.items()\n",
    ")\n",
    "\n",
    "print(f\"\\nRencana augmentasi untuk kelas {CLASS_NAMES[minority_class_id]}:\")\n",
    "print(f\"  - Jumlah gambar unik yang ditambah: {len(AUGMENTATION_PLAN)}\")\n",
    "print(f\"  - Total augmentasi baru: {sum(AUGMENTATION_PLAN.values())}\")\n",
    "print(f\"  - Perkiraan penambahan instance minoritas: {expected_new_minority}\")\n",
    "if balance_deficit > 0 and not AUGMENTATION_PLAN:\n",
    "    print(\"  ! Tidak ada gambar minoritas yang tersedia untuk di-augment. Dataset tetap tidak seimbang.\")\n",
    "\n",
    "BASELINE_TRAIN_COUNTS = train_counts.copy()\n",
    "MINORITY_CLASS_ID = minority_class_id\n",
    "MAJORITY_CLASS_ID = majority_class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87305a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk melakukan augmentasi copy-paste pada segmentation dataset\n",
    "def augment_segmentation_dataset(\n",
    "    images_path,\n",
    "    labels_path,\n",
    "    base_num_augmentations=0,\n",
    "    augmentation_plan=None,\n",
    "    minority_class_id=0,\n",
    "    class_names=None,\n",
    "    apply_color_aug=False,\n",
    "    mask_blur_sigma=3,\n",
    "    min_objects_per_paste=1,\n",
    "    max_objects_per_paste=3,\n",
    "    padding=4,\n",
    "):\n",
    "    \"\"\"Generate balanced data using copy-paste augmentation for YOLO segmentation.\"\"\"\n",
    "\n",
    "    images_dir = Path(images_path)\n",
    "    labels_dir = Path(labels_path)\n",
    "    augmentation_plan = augmentation_plan or {}\n",
    "\n",
    "    image_files = sorted(list(images_dir.glob('*.jpg')) + list(images_dir.glob('*.jpeg')) + list(images_dir.glob('*.png')))\n",
    "    if not image_files:\n",
    "        print(\"Tidak ada gambar ditemukan di direktori train.\")\n",
    "        return 0\n",
    "\n",
    "    stem_to_path = {img_path.stem: img_path for img_path in image_files}\n",
    "    label_exists = {stem: (labels_dir / f\"{stem}.txt\").exists() for stem in stem_to_path}\n",
    "    base_stems = [stem for stem, exists in label_exists.items() if exists]\n",
    "\n",
    "    if not base_stems:\n",
    "        print(\"Tidak ada file label YOLO yang ditemukan. Augmentasi dibatalkan.\")\n",
    "        return 0\n",
    "\n",
    "    class_names = class_names or []\n",
    "\n",
    "    augmented_images = 0\n",
    "    total_pasted_instances = 0\n",
    "\n",
    "    for stem, donor_img_path in stem_to_path.items():\n",
    "        total_aug = base_num_augmentations + augmentation_plan.get(stem, 0)\n",
    "        if total_aug <= 0:\n",
    "            continue\n",
    "\n",
    "        donor_label_path = labels_dir / f\"{stem}.txt\"\n",
    "        if not donor_label_path.exists():\n",
    "            continue\n",
    "\n",
    "        donor_annotations = read_yolo_annotation(donor_label_path)\n",
    "        minority_annotations = [ann for ann in donor_annotations if int(ann[0]) == minority_class_id]\n",
    "        if not minority_annotations:\n",
    "            continue\n",
    "\n",
    "        donor_bgr = cv2.imread(str(donor_img_path))\n",
    "        if donor_bgr is None:\n",
    "            print(f\"Gagal membaca gambar donor: {donor_img_path}\")\n",
    "            continue\n",
    "        donor_rgb = cv2.cvtColor(donor_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        for aug_idx in range(total_aug):\n",
    "            base_stem = random.choice(base_stems)\n",
    "            base_img_path = stem_to_path[base_stem]\n",
    "            base_label_path = labels_dir / f\"{base_stem}.txt\"\n",
    "\n",
    "            base_annotations = read_yolo_annotation(base_label_path)\n",
    "            base_bgr = cv2.imread(str(base_img_path))\n",
    "            if base_bgr is None:\n",
    "                print(f\"Gagal membaca gambar target: {base_img_path}\")\n",
    "                continue\n",
    "            base_rgb = cv2.cvtColor(base_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            new_image = base_rgb.copy()\n",
    "            new_annotations = [list(ann) for ann in base_annotations]\n",
    "            pasted_this_image = 0\n",
    "\n",
    "            max_pick = min(max_objects_per_paste, len(minority_annotations))\n",
    "            min_pick = min(min_objects_per_paste, max_pick)\n",
    "            if max_pick <= 0 or min_pick <= 0:\n",
    "                continue\n",
    "\n",
    "            num_to_paste = random.randint(min_pick, max_pick)\n",
    "            selected_objects = random.sample(minority_annotations, num_to_paste)\n",
    "\n",
    "            for obj in selected_objects:\n",
    "                result = extract_object_patch(donor_rgb, obj[1:], padding=padding)\n",
    "                if result is None:\n",
    "                    continue\n",
    "                patch, mask_patch, polygon_local = result\n",
    "                new_polygon = paste_patch_on_base(\n",
    "                    new_image,\n",
    "                    patch,\n",
    "                    mask_patch,\n",
    "                    polygon_local,\n",
    "                    sigma=mask_blur_sigma,\n",
    "                )\n",
    "                if new_polygon is None:\n",
    "                    continue\n",
    "                new_annotations.append([obj[0]] + new_polygon)\n",
    "                pasted_this_image += 1\n",
    "                total_pasted_instances += 1\n",
    "\n",
    "            if pasted_this_image == 0:\n",
    "                continue\n",
    "\n",
    "            if apply_color_aug:\n",
    "                augmented = transform_color(image=new_image)\n",
    "                new_image = augmented['image']\n",
    "\n",
    "            base_suffix = base_img_path.suffix or '.jpg'\n",
    "            new_stem = f\"{base_stem}_cp_{stem}_{aug_idx}\"\n",
    "            unique_id = 0\n",
    "            new_image_path = images_dir / f\"{new_stem}{base_suffix}\"\n",
    "            new_label_path = labels_dir / f\"{new_stem}.txt\"\n",
    "\n",
    "            while new_image_path.exists() or new_label_path.exists():\n",
    "                unique_id += 1\n",
    "                new_stem = f\"{base_stem}_cp_{stem}_{aug_idx}_{unique_id}\"\n",
    "                new_image_path = images_dir / f\"{new_stem}{base_suffix}\"\n",
    "                new_label_path = labels_dir / f\"{new_stem}.txt\"\n",
    "\n",
    "            cv2.imwrite(str(new_image_path), cv2.cvtColor(new_image, cv2.COLOR_RGB2BGR))\n",
    "            write_yolo_annotation(new_label_path, new_annotations)\n",
    "            augmented_images += 1\n",
    "\n",
    "    class_name = class_names[minority_class_id] if class_names and minority_class_id < len(class_names) else minority_class_id\n",
    "    print(\n",
    "        f\"\\nSelesai! Total {augmented_images} gambar baru telah dibuat dengan copy-paste.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Total instance kelas {class_name} yang ditempel: {total_pasted_instances}\"\n",
    "    )\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "print(\"Fungsi augment_segmentation_dataset copy-paste berhasil didefinisikan!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jalankan augmentasi copy-paste\n",
    "print(\"Memulai proses augmentasi copy-paste dengan balancing kelas...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "augmented_count = augment_segmentation_dataset(\n",
    "    images_path=TRAIN_IMAGES_PATH,\n",
    "    labels_path=TRAIN_LABELS_PATH,\n",
    "    base_num_augmentations=DEFAULT_BASE_AUGMENTATIONS,\n",
    "    augmentation_plan=AUGMENTATION_PLAN,\n",
    "    minority_class_id=MINORITY_CLASS_ID,\n",
    "    class_names=CLASS_NAMES,\n",
    "    apply_color_aug=APPLY_COLOR_AUG,\n",
    "    mask_blur_sigma=MASK_BLUR_SIGMA,\n",
    "    min_objects_per_paste=COPY_PASTE_MIN_OBJECTS,\n",
    "    max_objects_per_paste=COPY_PASTE_MAX_OBJECTS,\n",
    "    padding=COPY_PASTE_PADDING,\n",
    ")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Augmentasi selesai!\")\n",
    "print(f\"Total gambar baru: {augmented_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27785d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifikasi hasil augmentasi dan balancing\n",
    "train_counts_after, _ = collect_class_stats(TRAIN_LABELS_PATH, len(CLASS_NAMES))\n",
    "\n",
    "if VAL_LABELS_PATH:\n",
    "    val_counts_after, _ = collect_class_stats(VAL_LABELS_PATH, len(CLASS_NAMES))\n",
    "else:\n",
    "    val_counts_after = Counter({cls: 0 for cls in range(len(CLASS_NAMES))})\n",
    "\n",
    "if TEST_LABELS_PATH:\n",
    "    test_counts_after, _ = collect_class_stats(TEST_LABELS_PATH, len(CLASS_NAMES))\n",
    "else:\n",
    "    test_counts_after = Counter({cls: 0 for cls in range(len(CLASS_NAMES))})\n",
    "\n",
    "print(\"Statistik Dataset Setelah Augmentasi:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Path dataset: {DATASET_PATH}\")\n",
    "print(f\"Train images path: {TRAIN_IMAGES_PATH}\")\n",
    "print(f\"Train labels path: {TRAIN_LABELS_PATH}\")\n",
    "\n",
    "if 'BASELINE_TRAIN_COUNTS' in globals():\n",
    "    print(\"\\nPerbandingan distribusi kelas (train):\")\n",
    "    for idx, class_name in enumerate(CLASS_NAMES):\n",
    "        before = BASELINE_TRAIN_COUNTS.get(idx, 0)\n",
    "        after = train_counts_after.get(idx, 0)\n",
    "        delta = after - before\n",
    "        sign = \"+\" if delta >= 0 else \"\"\n",
    "        print(f\"  - {class_name}: sebelum={before}, sesudah={after} ({sign}{delta})\")\n",
    "else:\n",
    "    print_class_distribution(\"Train (setelah augmentasi)\", train_counts_after, CLASS_NAMES)\n",
    "\n",
    "if VAL_LABELS_PATH:\n",
    "    print_class_distribution(\"Validation\", val_counts_after, CLASS_NAMES)\n",
    "if TEST_LABELS_PATH:\n",
    "    print_class_distribution(\"Test\", test_counts_after, CLASS_NAMES)\n",
    "\n",
    "print(\"\\nContoh file hasil augmentasi:\")\n",
    "aug_files = [\n",
    "    f for f in sorted(TRAIN_IMAGES_PATH.glob('*'))\n",
    "    if '_aug' in f.stem and f.suffix.lower() in {'.jpg', '.jpeg', '.png'}\n",
    "]\n",
    "for i, path in enumerate(aug_files[:5]):\n",
    "    print(f\"  {i+1}. {path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacc89be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan visualisasi hasil augmentasi\n",
    "visualize_augmentations(TRAIN_IMAGES_PATH, num_samples=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_lab1 (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
